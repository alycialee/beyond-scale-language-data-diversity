# Beyond Scale II: A Systematic study of the role of Data Diversity on Large Language Models' Test Performance

Brando Miranda, contact email: brando9@stanford.edu
AI/ML
Aut_win_spr, 2023-2024 Academic Year
Course credit
Up to 5 students

# Project Description
This project aims to systematically explore the role of data diversity in enhancing the test performance of Large Language Models (LLMs) across specific and general benchmarks, 
such as reasoning, writing technical articles, autoformalization, theorem proving, and coding. 
We hypothesize that data diversity likely improves performance on general benchmarks by increasing the convex hull and thus, the benefits of transfer learning. 
We hypothesize that alignment of the source data with the target eval also matters.
The project will involve a investigation of formal quantitative measures of diversity, like the Task2Vec diversity coefficient, and their correlation/causation with test performance. 
By focusing on data-centric tools and methodologies, we aspire to automate the data selection pipeline for improved data efficiency,
potentially breaking existing scaling laws in the landscape of LLMs like GPT-4.

# Recommended Background
Candidates are encouraged to share their background when reaching out. 
A strong foundation in Python is essential, and a belief in the paramount role of data in Machine Learning, aligning with data-centric ML principles, is a plus.

Key citations:
- Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data: https://arxiv.org/abs/2306.13840
- The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence: https://slideslive.com/38996684/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity
- Task2Vec: Task Embedding for Meta-Learning https://arxiv.org/abs/1902.03545
- Beyond neural scaling laws: beating power law scaling via data pruning: https://arxiv.org/abs/2206.14486

# Prerequisites / Preparation
Participants are expected to make direct contributions to the project and should be comfortable coding in Python. 
A passion for exploring the intricacies of data and its pivotal role in the development and efficiency of machine learning models will be advantageous.

# Motivation
This endeavor is driven by the aspiration to unravel the profound implications of data quality and diversity in the realm of Large Language Models, contributing to advancements in AI/ML and paving the way for more robust and efficient models capable of unprecedented capabilities in various domains. 
By delving into the essence of data-centric approaches, we hope to inspire a new wave of research focused on optimizing the symbiosis between data and model, pushing the boundaries of what is achievable in artificial intelligence.

